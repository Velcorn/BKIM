{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "if (!require(\"BiocManager\", quietly = TRUE))\n",
    "    install.packages(\"BiocManager\")\n",
    "BiocManager::install(version = \"3.16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "BiocManager::install(\"splatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Load package\n",
    "suppressPackageStartupMessages({\n",
    "  library(splatter)\n",
    "  library(scater)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Pre-processing\n",
    "### 1.1) Simulate 5000 samples with 2000 genes from six cell types, with the ratio of 5, 10, 10, 20, 25, 30 (fairly imbalanced), and load the samples in Torch tensors.\n",
    "Please use values for other parameters in a way that simplifies the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uP2cwOtI-wbp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sim <- splatSimulate(method=\"groups\", nGenes = 2000, batchCells = 5000 , mean.rate = 0.6,mean.rate = 0.6, dropout.mid = c(6,3,6,3,6,3), dropout.shape=c(-1,-1,-1,-1,-1,-1), dropout.type=\"group\", group.prob = c(0.05, 0.1, 0.1, 0.2, 0.25, 0.3), verbose=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "write.table(as.array(counts(sim)), sep='\\t', 'data.tsv')\n",
    "write.table(colData(sim), sep='\\t', 'labels.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data.tsv', sep='\\t').transpose().to_numpy()\n",
    "labels = pd.read_csv('labels.tsv', sep='\\t')[\"Group\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing.normalize(data)\n",
    "labels = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).to(torch.float32)\n",
    "y_train = torch.from_numpy(y_train).to(torch.long)\n",
    "X_test = torch.from_numpy(X_test).to(torch.float32)\n",
    "y_test = torch.from_numpy(y_test).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB4euiyELR_b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2) Use the model in exercise 4 on the data to classify RNA samples into one of the six cell types. Split the data to train and test sets with a proportionate number of samples in the train and test set. \n",
    "### 2.1) Train the model using SGD, Cross entropy loss\n",
    "\n",
    "Please remember to load the data loader, instantiate the model, optimizer, and loss, and implement the training loop. Please use enough number epochs and proper batch-size, and learning rate to improve the model convergence.\n",
    "Please calculate test and train loss and accuracy. Also, calculate the test AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "lE8q8iAYKxCd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train) \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test) \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(500, 6),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2000)\n",
    "        output = self.layers(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, log_interval=50):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    size = len(train_loader.dataset)\n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            correct += (output.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Loss: 1.7928946018218994\n",
      "Train Loss: 1.7802987098693848\n",
      "Train Loss: 1.774113416671753\n",
      "Test Loss: 0.056557815194129946\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Loss: 1.780104398727417\n",
      "Train Loss: 1.75491201877594\n",
      "Train Loss: 1.7663451433181763\n",
      "Test Loss: 0.055753186106681826\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Loss: 1.7416731119155884\n",
      "Train Loss: 1.6988426446914673\n",
      "Train Loss: 1.7150121927261353\n",
      "Test Loss: 0.0551405885219574\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Loss: 1.7629846334457397\n",
      "Train Loss: 1.6390866041183472\n",
      "Train Loss: 1.6644062995910645\n",
      "Test Loss: 0.054883543133735654\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Loss: 1.7128229141235352\n",
      "Train Loss: 1.6971194744110107\n",
      "Train Loss: 1.7092393636703491\n",
      "Test Loss: 0.054717058181762694\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Loss: 1.618843913078308\n",
      "Train Loss: 1.7510368824005127\n",
      "Train Loss: 1.7558908462524414\n",
      "Test Loss: 0.05457686364650726\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Loss: 1.7153979539871216\n",
      "Train Loss: 1.7180551290512085\n",
      "Train Loss: 1.7417761087417603\n",
      "Test Loss: 0.054395997166633604\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Loss: 1.6992223262786865\n",
      "Train Loss: 1.7257155179977417\n",
      "Train Loss: 1.694894790649414\n",
      "Test Loss: 0.054095875024795535\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Loss: 1.6181100606918335\n",
      "Train Loss: 1.6824417114257812\n",
      "Train Loss: 1.6823283433914185\n",
      "Test Loss: 0.053591938376426694\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Loss: 1.6838316917419434\n",
      "Train Loss: 1.660852074623108\n",
      "Train Loss: 1.6123297214508057\n",
      "Test Loss: 0.05216066169738769\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Loss: 1.7188249826431274\n",
      "Train Loss: 1.5374681949615479\n",
      "Train Loss: 1.5316028594970703\n",
      "Test Loss: 0.049960127234458925\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Loss: 1.664082407951355\n",
      "Train Loss: 1.417229175567627\n",
      "Train Loss: 1.5432361364364624\n",
      "Test Loss: 0.048678613543510434\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Loss: 1.5221798419952393\n",
      "Train Loss: 1.6177189350128174\n",
      "Train Loss: 1.5292710065841675\n",
      "Test Loss: 0.048290067195892336\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Loss: 1.4469102621078491\n",
      "Train Loss: 1.5219271183013916\n",
      "Train Loss: 1.4657671451568604\n",
      "Test Loss: 0.04807058084011078\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Loss: 1.270540714263916\n",
      "Train Loss: 1.273881435394287\n",
      "Train Loss: 1.519880771636963\n",
      "Test Loss: 0.047943779945373535\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Loss: 1.4530104398727417\n",
      "Train Loss: 1.4579452276229858\n",
      "Train Loss: 1.516013503074646\n",
      "Test Loss: 0.047887047290802\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Loss: 1.57301664352417\n",
      "Train Loss: 1.6116231679916382\n",
      "Train Loss: 1.6065802574157715\n",
      "Test Loss: 0.04784774601459503\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Loss: 1.6366554498672485\n",
      "Train Loss: 1.4798824787139893\n",
      "Train Loss: 1.452521562576294\n",
      "Test Loss: 0.04781139969825745\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Loss: 1.5431668758392334\n",
      "Train Loss: 1.421342134475708\n",
      "Train Loss: 1.6371097564697266\n",
      "Test Loss: 0.047797009348869324\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Loss: 1.5793561935424805\n",
      "Train Loss: 1.389038324356079\n",
      "Train Loss: 1.2952131032943726\n",
      "Test Loss: 0.047774983525276184\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Loss: 1.5504034757614136\n",
      "Train Loss: 1.360406756401062\n",
      "Train Loss: 1.517724871635437\n",
      "Test Loss: 0.04778965616226196\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Loss: 1.4528065919876099\n",
      "Train Loss: 1.5439108610153198\n",
      "Train Loss: 1.5428096055984497\n",
      "Test Loss: 0.04776698660850525\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Loss: 1.51300048828125\n",
      "Train Loss: 1.4196200370788574\n",
      "Train Loss: 1.517303466796875\n",
      "Test Loss: 0.04779155492782593\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Loss: 1.6658732891082764\n",
      "Train Loss: 1.5736501216888428\n",
      "Train Loss: 1.4192866086959839\n",
      "Test Loss: 0.04777658712863922\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Loss: 1.5438435077667236\n",
      "Train Loss: 1.583012342453003\n",
      "Train Loss: 1.3877469301223755\n",
      "Test Loss: 0.04774863159656525\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Loss: 1.6054250001907349\n",
      "Train Loss: 1.387404441833496\n",
      "Train Loss: 1.3890076875686646\n",
      "Test Loss: 0.04773689913749695\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Loss: 1.4811999797821045\n",
      "Train Loss: 1.5733308792114258\n",
      "Train Loss: 1.5718046426773071\n",
      "Test Loss: 0.04775558936595917\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Loss: 1.5148521661758423\n",
      "Train Loss: 1.6015523672103882\n",
      "Train Loss: 1.4489777088165283\n",
      "Test Loss: 0.047744178414344786\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Loss: 1.2927342653274536\n",
      "Train Loss: 1.4173650741577148\n",
      "Train Loss: 1.4470384120941162\n",
      "Test Loss: 0.0477342756986618\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Loss: 1.4477324485778809\n",
      "Train Loss: 1.510177493095398\n",
      "Train Loss: 1.3554952144622803\n",
      "Test Loss: 0.04770999252796173\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train Loss: 1.4820657968521118\n",
      "Train Loss: 1.514465093612671\n",
      "Train Loss: 1.4848580360412598\n",
      "Test Loss: 0.047712112188339234\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train Loss: 1.420691967010498\n",
      "Train Loss: 1.4161853790283203\n",
      "Train Loss: 1.4801439046859741\n",
      "Test Loss: 0.04773529529571533\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train Loss: 1.54049551486969\n",
      "Train Loss: 1.6058355569839478\n",
      "Train Loss: 1.3882211446762085\n",
      "Test Loss: 0.04771805620193482\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train Loss: 1.6320252418518066\n",
      "Train Loss: 1.3858462572097778\n",
      "Train Loss: 1.5992391109466553\n",
      "Test Loss: 0.04772380876541138\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train Loss: 1.3902925252914429\n",
      "Train Loss: 1.3242138624191284\n",
      "Train Loss: 1.5712311267852783\n",
      "Test Loss: 0.04770383095741272\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train Loss: 1.4498796463012695\n",
      "Train Loss: 1.513084053993225\n",
      "Train Loss: 1.3566365242004395\n",
      "Test Loss: 0.04772137129306793\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train Loss: 1.5070561170578003\n",
      "Train Loss: 1.540420413017273\n",
      "Train Loss: 1.4200738668441772\n",
      "Test Loss: 0.04770772933959961\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train Loss: 1.4499768018722534\n",
      "Train Loss: 1.4507218599319458\n",
      "Train Loss: 1.476351022720337\n",
      "Test Loss: 0.04771992230415344\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train Loss: 1.4518511295318604\n",
      "Train Loss: 1.5394121408462524\n",
      "Train Loss: 1.5094923973083496\n",
      "Test Loss: 0.04772785985469818\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train Loss: 1.4476221799850464\n",
      "Train Loss: 1.4483771324157715\n",
      "Train Loss: 1.5998852252960205\n",
      "Test Loss: 0.04769260895252228\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train Loss: 1.4151012897491455\n",
      "Train Loss: 1.4744670391082764\n",
      "Train Loss: 1.44712233543396\n",
      "Test Loss: 0.04769688391685486\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train Loss: 1.5070077180862427\n",
      "Train Loss: 1.4137969017028809\n",
      "Train Loss: 1.417742133140564\n",
      "Test Loss: 0.04768628931045532\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train Loss: 1.452091932296753\n",
      "Train Loss: 1.509785532951355\n",
      "Train Loss: 1.5445886850357056\n",
      "Test Loss: 0.04768465840816498\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train Loss: 1.5429624319076538\n",
      "Train Loss: 1.5111194849014282\n",
      "Train Loss: 1.3520166873931885\n",
      "Test Loss: 0.04769030010700226\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train Loss: 1.4147050380706787\n",
      "Train Loss: 1.6002120971679688\n",
      "Train Loss: 1.29298734664917\n",
      "Test Loss: 0.047719435453414914\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train Loss: 1.4797576665878296\n",
      "Train Loss: 1.3234295845031738\n",
      "Train Loss: 1.4509735107421875\n",
      "Test Loss: 0.047691584587097165\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train Loss: 1.6004964113235474\n",
      "Train Loss: 1.5972918272018433\n",
      "Train Loss: 1.4493659734725952\n",
      "Test Loss: 0.047704061627388\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train Loss: 1.5723334550857544\n",
      "Train Loss: 1.5103964805603027\n",
      "Train Loss: 1.3852589130401611\n",
      "Test Loss: 0.04768131399154663\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train Loss: 1.5428763628005981\n",
      "Train Loss: 1.4800879955291748\n",
      "Train Loss: 1.3549625873565674\n",
      "Test Loss: 0.047702650785446166\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Loss: 1.4453129768371582\n",
      "Train Loss: 1.4178544282913208\n",
      "Train Loss: 1.3873913288116455\n",
      "Test Loss: 0.04769735670089722\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model)\n",
    "    test(model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzYIQfgGM7iP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2) Try to run the same model with Focal Loss to tackle the data imbalancedness. \n",
    "Please compare the results with the 2.1 results to show the impact of using focal loss on tackling data imbalancedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "model2 = Net().to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "criterion = sigmoid_focal_loss\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model2)\n",
    "    test(model2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gupsdAuBU_GH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3) Tackle over-fitting\n",
    "### 3.1) Use [Batch normalization layers](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) to improve the results. Compare the results with previous ones.\n",
    "\n",
    "For comparison, please draw AUC curve changes during different epochs (three curves for 2.1, 2.2, and 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7o89t_vVddo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScpmGp4aUCye",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4) Improve the results **(Bonus)**\n",
    "### 4.1) Use the internet resources to further improve the results by changing layers, architecture, optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6gCBCRvUiSS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
